#!/usr/bin/env python3
"""
FaceFusion Service Module
„Åπ„ÄÅÂà•„Å´„ÅÇ„Çì„Åü„ÅÆ„Åü„ÇÅ„Åò„ÇÉ„Å™„ÅÑ„Åë„Å©„ÄÅ„Å°„ÇÉ„Çì„Å®„Åó„Åü„Çµ„Éº„Éì„Çπ„ÇØ„É©„Çπ„Çí‰Ωú„Å£„Å¶„ÅÇ„Åí„Çã„Çè„ÇàÔºÅ
"""
import os
import sys
import subprocess
import tempfile
import shutil
import time
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
import torch

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FaceFusionService:
    """FaceFusionÂá¶ÁêÜ„Çµ„Éº„Éì„Çπ„ÇØ„É©„Çπ"""
    
    def __init__(self, facefusion_path: str = None):
        """
        ÂàùÊúüÂåñ
        Args:
            facefusion_path: FaceFusion„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´„Éë„Çπ
        """
        if facefusion_path is None:
            # Docker„Ç≥„É≥„ÉÜ„ÉäÂÜÖ„Åß„ÅÆÊ®ôÊ∫ñ„Éë„Çπ
            facefusion_path = "/app/facefusion"
            
        self.facefusion_path = Path(facefusion_path)
        self.setup_environment()
        self.validate_environment()
        
    def setup_environment(self):
        """Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö"""
        os.environ['OMP_NUM_THREADS'] = '1'
        os.environ['PYTHONPATH'] = str(self.facefusion_path)
        
        # CUDAË®≠ÂÆö
        if torch.cuda.is_available():
            os.environ['CUDA_VISIBLE_DEVICES'] = os.environ.get('CUDA_VISIBLE_DEVICES', '0')
            
    def validate_environment(self) -> Dict[str, Any]:
        """Áí∞Â¢ÉÊ§úË®º"""
        validation = {
            "facefusion_available": self.facefusion_path.exists(),
            "facefusion_script": (self.facefusion_path / "facefusion.py").exists(),
            "gpu_available": torch.cuda.is_available(),
            "cuda_version": None,
            "gpu_count": 0,
            "gpu_names": []
        }
        
        if torch.cuda.is_available():
            validation["cuda_version"] = torch.version.cuda
            validation["gpu_count"] = torch.cuda.device_count()
            validation["gpu_names"] = [
                torch.cuda.get_device_name(i) 
                for i in range(torch.cuda.device_count())
            ]
            
        logger.info(f"üîç Áí∞Â¢ÉÊ§úË®ºÁµêÊûú: {validation}")
        return validation
        
    def get_available_models(self) -> List[str]:
        """Âà©Áî®ÂèØËÉΩ„Å™„É¢„Éá„É´‰∏ÄË¶ß„ÇíÂèñÂæó"""
        return [
            "inswapper_128",
            "inswapper_128_fp16", 
            "blendswap_256",
            "ghost_1_256",
            "ghost_2_256",
            "ghost_3_256",
            "simswap_256",
            "uniface_256"
        ]
        
    def face_swap_image(
        self,
        source_path: str,
        target_path: str,
        output_path: str,
        model: str = "inswapper_128",
        quality: int = 90,
        pixel_boost: str = "128x128"
    ) -> Dict[str, Any]:
        """
        ÁîªÂÉè„ÅÆface swapÂá¶ÁêÜ
        
        Args:
            source_path: „ÇΩ„Éº„ÇπÁîªÂÉè„Éë„Çπ
            target_path: „Çø„Éº„Ç≤„ÉÉ„ÉàÁîªÂÉè„Éë„Çπ  
            output_path: Âá∫ÂäõÁîªÂÉè„Éë„Çπ
            model: ‰ΩøÁî®„Åô„Çã„É¢„Éá„É´
            quality: Âá∫ÂäõÂìÅË≥™
            pixel_boost: „Éî„ÇØ„Çª„É´„Éñ„Éº„Çπ„Éà
            
        Returns:
            Âá¶ÁêÜÁµêÊûúËæûÊõ∏
        """
        logger.info(f"üé≠ ÁîªÂÉèFace SwapÈñãÂßã: {Path(source_path).name} -> {Path(target_path).name}")
        
        # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # „Ç≥„Éû„É≥„ÉâÊßãÁØâ
        cmd = [
            'python3', 'facefusion.py', 'headless-run',
            '--source-paths', source_path,
            '--target-path', target_path,
            '--output-path', output_path,
            '--processors', 'face_swapper',
            '--face-swapper-model', model,
            '--face-swapper-pixel-boost', pixel_boost,
            '--execution-providers', 'cuda' if torch.cuda.is_available() else 'cpu',
            '--log-level', 'info',
            '--output-image-quality', str(quality)
        ]
        
        return self._execute_command(cmd, output_path, "image", model=model, quality=quality)
        
    def face_swap_video(
        self,
        source_path: str,
        target_path: str,
        output_path: str,
        model: str = "inswapper_128",
        quality: int = 80,
        pixel_boost: str = "128x128",
        trim_start: int = 0,
        trim_end: Optional[int] = None,
        max_frames: int = 100
    ) -> Dict[str, Any]:
        """
        ÂãïÁîª„ÅÆface swapÂá¶ÁêÜ
        
        Args:
            source_path: „ÇΩ„Éº„ÇπÁîªÂÉè„Éë„Çπ
            target_path: „Çø„Éº„Ç≤„ÉÉ„ÉàÂãïÁîª„Éë„Çπ
            output_path: Âá∫ÂäõÂãïÁîª„Éë„Çπ
            model: ‰ΩøÁî®„Åô„Çã„É¢„Éá„É´
            quality: Âá∫ÂäõÂìÅË≥™
            pixel_boost: „Éî„ÇØ„Çª„É´„Éñ„Éº„Çπ„Éà
            trim_start: ÈñãÂßã„Éï„É¨„Éº„É†
            trim_end: ÁµÇ‰∫Ü„Éï„É¨„Éº„É†
            max_frames: ÊúÄÂ§ß„Éï„É¨„Éº„É†Êï∞
            
        Returns:
            Âá¶ÁêÜÁµêÊûúËæûÊõ∏
        """
        logger.info(f"üé¨ ÂãïÁîªFace SwapÈñãÂßã: {Path(source_path).name} -> {Path(target_path).name}")
        
        # Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # „Éï„É¨„Éº„É†Âà∂Èôê
        if trim_end is None:
            trim_end = min(trim_start + max_frames, trim_start + 50)  # „Éá„Éï„Ç©„É´„Éà„Åß50„Éï„É¨„Éº„É†Âà∂Èôê
            
        # „Ç≥„Éû„É≥„ÉâÊßãÁØâ
        cmd = [
            'python3', 'facefusion.py', 'headless-run',
            '--source-paths', source_path,
            '--target-path', target_path,
            '--output-path', output_path,
            '--processors', 'face_swapper',
            '--face-swapper-model', model,
            '--face-swapper-pixel-boost', pixel_boost,
            '--execution-providers', 'cuda' if torch.cuda.is_available() else 'cpu',
            '--log-level', 'info',
            '--output-video-quality', str(quality),
            '--trim-frame-start', str(trim_start),
            '--trim-frame-end', str(trim_end)
        ]
            
        return self._execute_command(cmd, output_path, "video", model=model, quality=quality)
        
    def _execute_command(
        self, 
        cmd: list, 
        output_path: str, 
        media_type: str,
        model: str = None,
        quality: int = None
    ) -> Dict[str, Any]:
        """
        „Ç≥„Éû„É≥„ÉâÂÆüË°å
        
        Args:
            cmd: ÂÆüË°å„Ç≥„Éû„É≥„Éâ
            output_path: Âá∫Âäõ„Éë„Çπ
            media_type: „É°„Éá„Ç£„Ç¢„Çø„Ç§„Éó
            model: ‰ΩøÁî®„É¢„Éá„É´
            quality: ÂìÅË≥™
            
        Returns:
            ÂÆüË°åÁµêÊûúËæûÊõ∏
        """
        start_time = time.time()
        
        try:
            logger.info(f"üöÄ ÂÆüË°å„Ç≥„Éû„É≥„Éâ: {' '.join(cmd)}")
            
            # „Çø„Ç§„É†„Ç¢„Ç¶„ÉàË®≠ÂÆöÔºàÂãïÁîª„ÅØÈï∑„ÇÅÔºâ
            timeout = 1200 if media_type == "video" else 600
            
            # Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.facefusion_path)
            
            result = subprocess.run(
                cmd,
                cwd=self.facefusion_path,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=timeout,
                env=env
            )
            
            processing_time = time.time() - start_time
            
            # ÁµêÊûúÁ¢∫Ë™ç
            if os.path.exists(output_path):
                file_size = os.path.getsize(output_path)
                logger.info(f"‚úÖ {media_type} Face SwapÊàêÂäü: {file_size:,} bytes ({processing_time:.2f}Áßí)")
                
                return {
                    "success": True,
                    "output_path": output_path,
                    "file_size": file_size,
                    "media_type": media_type,
                    "processing_time": processing_time,
                    "model_used": model,
                    "quality": quality,
                    "return_code": result.returncode,
                    "message": f"{media_type} face swap completed successfully"
                }
            else:
                logger.error(f"‚ùå Âá∫Âäõ„Éï„Ç°„Ç§„É´„ÅåÁîüÊàê„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü: {output_path}")
                logger.error(f"üìã „É™„Çø„Éº„É≥„Ç≥„Éº„Éâ: {result.returncode}")
                logger.error(f"üì§ Ê®ôÊ∫ñÂá∫Âäõ: {result.stdout}")
                logger.error(f"üì§ „Ç®„É©„ÉºÂá∫Âäõ: {result.stderr}")
                return {
                    "success": False,
                    "error": "Output file not generated",
                    "return_code": result.returncode,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "processing_time": processing_time
                }
                
        except subprocess.TimeoutExpired:
            processing_time = time.time() - start_time
            logger.error(f"‚è∞ Âá¶ÁêÜ„Åå„Çø„Ç§„É†„Ç¢„Ç¶„Éà„Åó„Åæ„Åó„Åü ({timeout}Áßí)")
            return {
                "success": False,
                "error": f"Process timeout after {timeout} seconds",
                "processing_time": processing_time
            }
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"üí• ÂÆüË°å„Ç®„É©„Éº: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": processing_time
            }
                
    def validate_files(self, source_path: str, target_path: str) -> Dict[str, Any]:
        """
        „Éï„Ç°„Ç§„É´Ê§úË®º
        
        Args:
            source_path: „ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´„Éë„Çπ
            target_path: „Çø„Éº„Ç≤„ÉÉ„Éà„Éï„Ç°„Ç§„É´„Éë„Çπ
            
        Returns:
            Ê§úË®ºÁµêÊûúËæûÊõ∏
        """
        errors = []
        
        # „Éï„Ç°„Ç§„É´Â≠òÂú®Á¢∫Ë™ç
        if not os.path.exists(source_path):
            errors.append(f"„ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {source_path}")
            
        if not os.path.exists(target_path):
            errors.append(f"„Çø„Éº„Ç≤„ÉÉ„Éà„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {target_path}")
            
        # „Éï„Ç°„Ç§„É´Êã°ÂºµÂ≠êÁ¢∫Ë™ç
        source_ext = Path(source_path).suffix.lower()
        target_ext = Path(target_path).suffix.lower()
        
        valid_image_exts = ['.jpg', '.jpeg', '.png']
        valid_video_exts = ['.mp4', '.avi', '.mov']
        
        if source_ext not in valid_image_exts:
            errors.append(f"„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´ÂΩ¢Âºè: {source_ext}")
            
        if target_ext not in valid_image_exts + valid_video_exts:
            errors.append(f"„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Çø„Éº„Ç≤„ÉÉ„Éà„Éï„Ç°„Ç§„É´ÂΩ¢Âºè: {target_ext}")
            
        # „Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫Á¢∫Ë™çÔºà100MBÂà∂ÈôêÔºâ
        max_size = 100 * 1024 * 1024  # 100MB
        
        if os.path.exists(source_path):
            source_size = os.path.getsize(source_path)
            if source_size > max_size:
                errors.append(f"„ÇΩ„Éº„Çπ„Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫„ÅåÂ§ß„Åç„Åô„Åé„Åæ„Åô: {source_size / 1024 / 1024:.1f}MB")
                
        if os.path.exists(target_path):
            target_size = os.path.getsize(target_path)
            if target_size > max_size:
                errors.append(f"„Çø„Éº„Ç≤„ÉÉ„Éà„Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫„ÅåÂ§ß„Åç„Åô„Åé„Åæ„Åô: {target_size / 1024 / 1024:.1f}MB")
        
        return {
            "valid": len(errors) == 0,
            "errors": errors
        }
        
    def get_system_info(self) -> Dict[str, Any]:
        """„Ç∑„Çπ„ÉÜ„É†ÊÉÖÂ†±ÂèñÂæó"""
        import platform
        import psutil
        
        info = {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "torch_version": torch.__version__ if torch else "N/A",
            "cuda_available": torch.cuda.is_available() if torch else False,
            "gpu_count": 0,
            "gpu_names": [],
            "memory_total": psutil.virtual_memory().total if psutil else None,
            "memory_available": psutil.virtual_memory().available if psutil else None
        }
        
        if torch and torch.cuda.is_available():
            info["cuda_version"] = torch.version.cuda
            info["gpu_count"] = torch.cuda.device_count()
            info["gpu_names"] = [
                torch.cuda.get_device_name(i) 
                for i in range(torch.cuda.device_count())
            ]
            
        return info
