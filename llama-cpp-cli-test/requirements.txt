# llama-cpp-python GPU CLI テスト環境
# CUDA 11.8対応

# Core dependencies
llama-cpp-python[server]==0.2.90
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0

# Additional utilities
python-multipart==0.0.6
python-dotenv==1.0.0
requests==2.31.0
aiofiles==23.2.1

# Development dependencies
pytest==7.4.3
pytest-asyncio==0.21.1
httpx==0.25.2

# Note: For GPU support, install with:
# CMAKE_ARGS="-DGGML_CUDA=on" FORCE_CMAKE=1 pip install llama-cpp-python[server] --no-cache-dir 