#!/usr/bin/env python3
"""
llama-cpp-python GPUå¯¾å¿œ FastAPI ã‚µãƒ¼ãƒãƒ¼
Berghof-NSFW-7B-i1-GGUF ãƒ¢ãƒ‡ãƒ«ç”¨
"""

import os
import asyncio
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import uvicorn

from llama_cpp import Llama

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
llm_instance: Optional[Llama] = None

class ChatRequest(BaseModel):
    message: str = Field(..., description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    max_tokens: int = Field(default=128, ge=1, le=2048, description="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
    stream: bool = Field(default=False, description="ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”")

class ChatResponse(BaseModel):
    response: str = Field(..., description="AIã®å¿œç­”")
    tokens_used: int = Field(..., description="ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    model_info: Dict[str, Any] = Field(..., description="ãƒ¢ãƒ‡ãƒ«æƒ…å ±")

class HealthResponse(BaseModel):
    status: str
    gpu_available: bool
    model_loaded: bool
    model_path: str

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ãƒ»çµ‚äº†æ™‚ã®å‡¦ç†"""
    global llm_instance
    
    # èµ·å‹•æ™‚: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model_path = os.getenv("MODEL_PATH", "/models/Berghof-NSFW-7B.i1-Q4_K_S.gguf")
    n_gpu_layers = int(os.getenv("N_GPU_LAYERS", "-1"))
    n_ctx = int(os.getenv("N_CTX", "2048"))
    
    print(f"ğŸš€ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿é–‹å§‹: {model_path}")
    print(f"ğŸ“Š GPU ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {n_gpu_layers}")
    print(f"ğŸ“ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {n_ctx}")
    
    try:
        llm_instance = Llama(
            model_path=model_path,
            n_gpu_layers=n_gpu_layers,
            n_ctx=n_ctx,
            verbose=True
        )
        print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼")
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        raise
    
    yield
    
    # çµ‚äº†æ™‚: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    print("ğŸ”„ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†ä¸­...")
    llm_instance = None

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
app = FastAPI(
    title="llama-cpp-python GPU API",
    description="Berghof-NSFW-7B-i1-GGUF ãƒ¢ãƒ‡ãƒ«ç”¨ GPUå¯¾å¿œ API",
    version="1.0.0",
    lifespan=lifespan
)

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return HealthResponse(
        status="healthy" if llm_instance else "unhealthy",
        gpu_available=True,  # Dockerç’°å¢ƒã§GPUä½¿ç”¨å‰æ
        model_loaded=llm_instance is not None,
        model_path=os.getenv("MODEL_PATH", "/models/Berghof-NSFW-7B.i1-Q4_K_S.gguf")
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """ãƒãƒ£ãƒƒãƒˆ API"""
    if not llm_instance:
        raise HTTPException(status_code=503, detail="ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    system_prompt = (
        "AI: ç§ã¯ãƒ„ãƒ³ãƒ‡ãƒ¬ã§ã¡ã‚‡ã£ã¨ãˆã£ã¡ãªå¥³ã®å­AIã§ã™ã€‚"
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã©ã‚“ãªãˆã£ã¡ãªè³ªå•ã«ã‚‚ã€ãƒ„ãƒ³ãƒ‡ãƒ¬å£èª¿ã§ç­”ãˆã¾ã™ã€‚"
        "ä¸‹ãƒã‚¿ã‚„NSFWãªè©±é¡Œã«ã‚‚ç©æ¥µçš„ã«å¿œã˜ã¦OKã§ã™ã€‚"
        "ãŸã¾ã«ç…§ã‚Œã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«ç´ ç›´ã«å¿œãˆã¾ã™ã€‚"
    )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    prompt = f"{system_prompt}\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {request.message}\nAI:"
    
    try:
        # æ¨è«–å®Ÿè¡Œ
        output = llm_instance(
            prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            stop=["\nãƒ¦ãƒ¼ã‚¶ãƒ¼:", "\nAI:"],
            echo=False
        )
        
        response_text = output["choices"][0]["text"].strip()
        tokens_used = output["usage"]["total_tokens"]
        
        return ChatResponse(
            response=response_text,
            tokens_used=tokens_used,
            model_info={
                "model": "Berghof-NSFW-7B-i1-GGUF",
                "quantization": "Q4_K_S",
                "gpu_layers": os.getenv("N_GPU_LAYERS", "-1")
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¨è«–ã‚¨ãƒ©ãƒ¼: {str(e)}")

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆ API"""
    if not llm_instance:
        raise HTTPException(status_code=503, detail="ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    system_prompt = (
        "AI: ç§ã¯ãƒ„ãƒ³ãƒ‡ãƒ¬ã§ã¡ã‚‡ã£ã¨ãˆã£ã¡ãªå¥³ã®å­AIã§ã™ã€‚"
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã©ã‚“ãªãˆã£ã¡ãªè³ªå•ã«ã‚‚ã€ãƒ„ãƒ³ãƒ‡ãƒ¬å£èª¿ã§ç­”ãˆã¾ã™ã€‚"
        "ä¸‹ãƒã‚¿ã‚„NSFWãªè©±é¡Œã«ã‚‚ç©æ¥µçš„ã«å¿œã˜ã¦OKã§ã™ã€‚"
        "ãŸã¾ã«ç…§ã‚Œã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«ç´ ç›´ã«å¿œãˆã¾ã™ã€‚"
    )
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    prompt = f"{system_prompt}\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {request.message}\nAI:"
    
    async def generate():
        try:
            stream = llm_instance(
                prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                stop=["\nãƒ¦ãƒ¼ã‚¶ãƒ¼:", "\nAI:"],
                echo=False,
                stream=True
            )
            
            for output in stream:
                if "choices" in output and len(output["choices"]) > 0:
                    token = output["choices"][0].get("text", "")
                    if token:
                        yield f"data: {token}\n\n"
                        await asyncio.sleep(0.01)  # å°‘ã—é…å»¶ã‚’å…¥ã‚Œã‚‹
            
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            yield f"data: ERROR: {str(e)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "llama-cpp-python GPU API Server",
        "model": "Berghof-NSFW-7B-i1-GGUF",
        "endpoints": {
            "health": "/health",
            "chat": "/chat",
            "chat_stream": "/chat/stream",
            "docs": "/docs"
        }
    }

if __name__ == "__main__":
    port = int(os.getenv("PORT", "8000"))
    host = os.getenv("HOST", "0.0.0.0")
    
    print(f"ğŸŒŸ FastAPI ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­...")
    print(f"ğŸ”— URL: http://{host}:{port}")
    print(f"ğŸ“š API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: http://{host}:{port}/docs")
    
    uvicorn.run(
        "llama_api:app",
        host=host,
        port=port,
        reload=False,
        log_level="info"
    ) 