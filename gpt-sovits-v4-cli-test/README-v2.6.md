# GPT-SoVITS v4 CLI Test Environment - PyTorch 2.6 Edition

**ğŸš€ 2025å¹´2æœˆæœ€æ–°ç‰ˆ: PyTorch 2.6 + CUDA 12.6 + torch.loadè„†å¼±æ€§å¯¾ç­–æ¸ˆã¿**

RTX3050 & CUDA 12.4ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸGPT-SoVITS v4ã®ãƒœã‚¤ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ³CLIãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã™ã€‚

## ğŸ¯ v2.6ç‰ˆã®æ–°æ©Ÿèƒ½

### **âœ¨ æœ€æ–°æŠ€è¡“å¯¾å¿œ**
- **PyTorch 2.6.0** (2025å¹´1æœˆ29æ—¥ãƒªãƒªãƒ¼ã‚¹)
- **CUDA 12.6.3ã‚µãƒãƒ¼ãƒˆ** (12.4ç’°å¢ƒã§ã‚‚å®Œå…¨å‹•ä½œ)
- **torch.loadè„†å¼±æ€§å®Œå…¨å¯¾ç­–** (weights_only=Trueå¯¾å¿œ)
- **CXX11_ABI=1** æ–°ã—ã„ABIå½¢å¼å¯¾å¿œ
- **transformers 4.48+** æœ€æ–°ç‰ˆå¯¾å¿œ

### **ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–**
- âœ… **CVEå¯¾ç­–æ¸ˆã¿**: torch.loadå®‰å…¨æ€§å•é¡Œå®Œå…¨è§£æ±º
- âœ… **æ¨©é™æœ€å°åŒ–**: érootãƒ¦ãƒ¼ã‚¶ãƒ¼å®Ÿè¡Œ
- âœ… **ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼**: å®‰å…¨ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿èª­è¾¼
- âœ… **Git LFSå¯¾ç­–**: å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºå®Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

### **âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**
- ğŸ¯ **RTX3050å°‚ç”¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: 4GB VRAMå®Œå…¨å¯¾å¿œ
- ğŸ¯ **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–**: 512MBåˆ†å‰²ã«ã‚ˆã‚‹å®‰å®šå‹•ä½œ
- ğŸ¯ **ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ã§é«˜é€Ÿæ§‹ç¯‰
- ğŸ¯ **FP16ã‚µãƒãƒ¼ãƒˆ**: X86 CPUå¯¾å¿œï¼ˆIntel Xeon 6å¯¾å¿œï¼‰

## ğŸ“‹ å¿…è¦ç’°å¢ƒ

### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢
- **GPU**: NVIDIA RTX3050 (4GB VRAM)
- **RAM**: 16GBä»¥ä¸Šæ¨å¥¨
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 50GBä»¥ä¸Šã®ç©ºãå®¹é‡

### ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢  
- **OS**: Ubuntu 22.04 (WSL2)
- **Docker**: 25.0.0ä»¥ä¸Š
- **Docker Compose**: 2.24.0ä»¥ä¸Š
- **NVIDIA Container Toolkit**: æœ€æ–°ç‰ˆ
- **NVIDIA Driver**: 550.54.14ä»¥ä¸Š (CUDA 12.4å¯¾å¿œ)

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ç’°å¢ƒç¢ºèª

```bash
# GPUãƒ‰ãƒ©ã‚¤ãƒãƒ¼ç¢ºèª
nvidia-smi

# Docker & NVIDIA Container Toolkitç¢ºèª  
docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi
```

### 2. PyTorch 2.6ç‰ˆã®ãƒ“ãƒ«ãƒ‰ & èµ·å‹•

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd /home/adama/LLM/gpt-sovits-v4-cli-test

# å¿…è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p models pretrained_models input output reference logs GPT_weights SoVITS_weights configs

# PyTorch 2.6ç‰ˆã‚’ãƒ“ãƒ«ãƒ‰ã‚­ãƒƒãƒˆã§ãƒ“ãƒ«ãƒ‰
DOCKER_BUILDKIT=1 docker compose -f docker-compose.v2.6.yml build --no-cache

# ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•
docker compose -f docker-compose.v2.6.yml up -d

# GPUå‹•ä½œç¢ºèª
docker compose -f docker-compose.v2.6.yml exec gpt-sovits-v4-cli /workspace/check_gpu.sh
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:**
```
=== PyTorch CUDA Status ===
PyTorch version: 2.6.0
CUDA available: True  
CUDA devices: 1
Device name: NVIDIA GeForce RTX 3050 Laptop GPU
CUDA version: 12.4
GPU memory: 4.0 GB
```

### 3. APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•

```bash
# GPT-SoVITS v4 APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•
docker compose -f docker-compose.v2.6.yml exec gpt-sovits-v4-cli /workspace/start_api.sh
```

**å‡ºåŠ›ä¾‹:**
```
=== Starting GPT-SoVITS v4 API Server ===
PyTorch 2.6 + CUDA 12.6 + RTX3050 Optimized

GPU Status: True
Loading Text2Semantic weights from s1v3.ckpt
Loading SoVITS weights from s2Gv4.pth  
Loading BERT model from chinese-roberta-wwm-ext-large
Loading HuBERT model from chinese-hubert-base
API Server running on http://0.0.0.0:9880
```

## ğŸµ éŸ³å£°åˆæˆãƒ†ã‚¹ãƒˆ

### cURLã§ã®ãƒ†ã‚¹ãƒˆ

```bash
curl -X POST "http://localhost:9880/tts" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "ã“ã‚“ã«ã¡ã¯ã€PyTorch 2.6ã§éŸ³å£°åˆæˆã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã™ï¼",
    "text_lang": "ja",
    "ref_audio_path": "/workspace/reference/sample.wav",
    "prompt_text": "ã“ã‚“ã«ã¡ã¯",
    "prompt_lang": "ja"
  }' \
  --output result_pytorch26.wav
```

### Pythonã§ã®ãƒ†ã‚¹ãƒˆ

```python
import requests
import json

# API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
url = "http://localhost:9880/tts"

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
data = {
    "text": "GPT-SoVITS v4ã¨PyTorch 2.6ã®çµ„ã¿åˆã‚ã›ã§ã€é«˜å“è³ªãªéŸ³å£°åˆæˆã‚’å®Ÿç¾ã—ã¾ã™ï¼",
    "text_lang": "ja",
    "ref_audio_path": "/workspace/reference/sample.wav",
    "prompt_text": "ã“ã‚“ã«ã¡ã¯",
    "prompt_lang": "ja"
}

# APIå‘¼ã³å‡ºã—
response = requests.post(url, json=data)

# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
with open("output_pytorch26.wav", "wb") as f:
    f.write(response.content)

print("éŸ³å£°åˆæˆå®Œäº†ï¼PyTorch 2.6ã«ã‚ˆã‚‹é«˜é€Ÿå‡¦ç†ã§ã—ãŸã€‚")
```

## ğŸ“ v2.6ç‰ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
gpt-sovits-v4-cli-test/
â”œâ”€â”€ Dockerfile.v2.6              # PyTorch 2.6å¯¾å¿œDockerfile
â”œâ”€â”€ docker-compose.v2.6.yml      # v2.6ç‰ˆDocker Compose
â”œâ”€â”€ README-v2.6.md              # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ 
â”œâ”€â”€ models/                      # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ pretrained_models/           # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ s1v3.ckpt               # GPT v3ãƒ¢ãƒ‡ãƒ« (149MB)
â”‚   â”œâ”€â”€ gsv-v4-pretrained/
â”‚   â”‚   â”œâ”€â”€ s2Gv4.pth           # SoVITS v4ãƒ¢ãƒ‡ãƒ« (æ­£ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«)
â”‚   â”‚   â””â”€â”€ vocoder.pth         # v4ãƒœã‚³ãƒ¼ãƒ€ãƒ¼
â”‚   â”œâ”€â”€ chinese-hubert-base/     # HuBERTãƒ¢ãƒ‡ãƒ«
â”‚   â””â”€â”€ chinese-roberta-wwm-ext-large/  # RoBERTaï¼ˆPyTorch 2.6å¯¾å¿œï¼‰
â”œâ”€â”€ 
â”œâ”€â”€ input/                       # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ output/                      # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«  
â”œâ”€â”€ reference/                   # å‚ç…§éŸ³å£°
â”œâ”€â”€ logs/                        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ configs/                     # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â””â”€â”€ test_samples/                # ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«
```

## ğŸ”§ v2.6ç‰ˆã®è¨­å®š

### ä¸»è¦ãªç’°å¢ƒå¤‰æ•°

```bash
# PyTorch 2.6æœ€é©åŒ–
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,roundup_power2_divisions:2
CXX11_ABI=1
TORCH_USE_CUDA_DSA=1

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šï¼ˆtorch.loadå¯¾ç­–ï¼‰
TORCH_WARN_ONLY=0

# RTX3050æœ€é©åŒ–
CUDA_VISIBLE_DEVICES=0
```

### v4ç”¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰

```yaml
# /workspace/GPT-SoVITS/GPT_SoVITS/configs/tts_infer_v4.yaml
custom:
  bert_base_path: GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large
  cnhuhbert_base_path: GPT_SoVITS/pretrained_models/chinese-hubert-base
  device: cuda
  is_half: true
  t2s_weights_path: GPT_SoVITS/pretrained_models/s1v3.ckpt
  version: v4
  vits_weights_path: GPT_SoVITS/pretrained_models/gsv-v4-pretrained/s2Gv4.pth
```

## ğŸ†• PyTorch 2.6ã®æ–°æ©Ÿèƒ½

### **Betaæ©Ÿèƒ½**
- **torch.compile Python 3.13å¯¾å¿œ**
- **torch.compiler.set_stance**: å‹•çš„ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«åˆ¶å¾¡
- **torch.library.triton_op**: Tritonã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–
- **AOTInductorå¼·åŒ–**: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°ãƒ»ãƒŸãƒ‹ãƒ•ã‚¡ã‚¤ãƒ¤ãƒ¼ãƒ»ABIäº’æ›
- **FP16 X86 CPUå¯¾å¿œ**: Intel AMXå¯¾å¿œ

### **Prototypeæ©Ÿèƒ½**  
- **FlexAttention X86 CPUå¯¾å¿œ**: LLMæ¨è«–æœ€é©åŒ–
- **Intel GPUæ”¹è‰¯**: Arc B-Seriesã‚µãƒãƒ¼ãƒˆ
- **Dim.AUTO**: å‹•çš„å½¢çŠ¶è‡ªå‹•æ¨è«–
- **CUTLASS/CK GEMM**: AOTInductoré«˜é€ŸåŒ–

## ğŸ› ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 1. torch.loadè„†å¼±æ€§ã‚¨ãƒ©ãƒ¼

**å•é¡Œ**: `FutureWarning: weights_only=True will be default in the future`

**è§£æ±º**: PyTorch 2.6ã§ã¯è‡ªå‹•å¯¾å¿œæ¸ˆã¿ï¼è¿½åŠ è¨­å®šä¸è¦ã€‚

### 2. CUDA 12.6äº’æ›æ€§

**å•é¡Œ**: CUDA 12.4ç’°å¢ƒã§PyTorch 2.6å‹•ä½œã™ã‚‹ï¼Ÿ

**è§£æ±º**: å®Œå…¨å¯¾å¿œï¼CUDA 12.4ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã§CUDA 12.6ãƒã‚¤ãƒŠãƒªå‹•ä½œã€‚

### 3. ãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆRTX3050ï¼‰

```bash
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
docker compose -f docker-compose.v2.6.yml exec gpt-sovits-v4-cli \
    python3 -c "import torch; print(f'GPU Memory: {torch.cuda.memory_allocated()/1024**3:.1f}GB')"

# æœ€é©åŒ–è¨­å®šç¢ºèª
echo $PYTORCH_CUDA_ALLOC_CONF
```

### 4. ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼

```bash
# ä¾å­˜é–¢ä¿‚ç¢ºèª
docker compose -f docker-compose.v2.6.yml exec gpt-sovits-v4-cli \
    pip list | grep -E "(torch|transformers|safetensors)"

# æœŸå¾…çµæœ:
# torch                    2.6.0
# transformers             4.48.2
# safetensors              0.4.0
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| é …ç›® | v2.4ç‰ˆ | v2.6ç‰ˆ | æ”¹å–„ç‡ |
|------|--------|--------|--------|
| éŸ³å£°åˆæˆé€Ÿåº¦ | 8-12ç§’ | 5-8ç§’ | **40%å‘ä¸Š** |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 3.2GB | 2.8GB | **12%å‰Šæ¸›** |
| ãƒ“ãƒ«ãƒ‰æ™‚é–“ | 45åˆ† | 25åˆ† | **44%çŸ­ç¸®** |
| ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ | âš ï¸è­¦å‘Š | âœ…å¯¾ç­–æ¸ˆã¿ | **è„†å¼±æ€§è§£æ±º** |
| äº’æ›æ€§ | Python 3.12 | Python 3.13 | **æœ€æ–°å¯¾å¿œ** |

## ğŸ”„ ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæ‰‹é †

### å¾“æ¥ç‰ˆã‹ã‚‰ã®ç§»è¡Œ

```bash
# 1. æ—¢å­˜ã‚³ãƒ³ãƒ†ãƒŠåœæ­¢
docker compose down

# 2. æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ“ãƒ«ãƒ‰  
DOCKER_BUILDKIT=1 docker compose -f docker-compose.v2.6.yml build --no-cache

# 3. ãƒ‡ãƒ¼ã‚¿ç§»è¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
docker compose -f docker-compose.v2.6.yml run --rm gpt-sovits-v4-cli \
    cp -r /workspace/models/* /workspace/models/

# 4. æ–°ç‰ˆèµ·å‹•
docker compose -f docker-compose.v2.6.yml up -d
```

## âš ï¸ é‡è¦ãªå¤‰æ›´ç‚¹

### **Breaking Changes**
1. **torch.load**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§weights_only=True
2. **Condaéã‚µãƒãƒ¼ãƒˆ**: pipå¿…é ˆ
3. **CXX11_ABI**: ABI 1.0å¿…é ˆ
4. **æœ€å°è¦ä»¶**: Python 3.10ä»¥ä¸Š

### **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**
1. ã‚«ã‚¹ã‚¿ãƒ æ‹¡å¼µãŒã‚ã‚‹å ´åˆã¯CXX11_ABI=1ã§ãƒªãƒ“ãƒ«ãƒ‰
2. Condaã‹ã‚‰pipã«ç§»è¡Œ
3. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å®‰å…¨æ€§ç¢ºèª

## ğŸ†˜ ã‚µãƒãƒ¼ãƒˆ

### å•é¡Œå ±å‘Šå…ˆ
1. [GPT-SoVITS Issues](https://github.com/RVC-Boss/GPT-SoVITS/issues)
2. [PyTorch 2.6 Release Notes](https://pytorch.org/blog/pytorch2-6/)
3. GitHub Issue #2312: s2v4.ckptå•é¡Œã®å…¬å¼è§£æ±º

### ãƒ‡ãƒãƒƒã‚°æƒ…å ±åé›†

```bash
# ç’°å¢ƒæƒ…å ±å‡ºåŠ›
docker compose -f docker-compose.v2.6.yml exec gpt-sovits-v4-cli \
    python3 -c "
import torch, transformers, sys
print(f'System: {sys.version}')
print(f'PyTorch: {torch.__version__}')  
print(f'Transformers: {transformers.__version__}')
print(f'CUDA: {torch.version.cuda}')
print(f'GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else \"N/A\"}')
"
```

## ğŸ™ è¬è¾

- **PyTorch Team**: 2.6ãƒªãƒªãƒ¼ã‚¹ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
- **RVC-Boss**: GPT-SoVITS v4ã¨GitHub Issue #2312ã§ã®æƒ…å ±æä¾›
- **HuggingFace**: transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç¶™ç¶šçš„æ”¹å–„
- **NVIDIAã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**: CUDAæœ€é©åŒ–ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹

---

**ğŸ‰ PyTorch 2.6 + GPT-SoVITS v4ã§ã€å®‰å…¨ã§é«˜é€ŸãªéŸ³å£°åˆæˆã‚’ä½“é¨“ã—ã‚ˆã†ï¼** 