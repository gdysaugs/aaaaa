#!/bin/bash
# ğŸ¤ GPT-SoVITS v4 ãƒ¢ãƒ‡ãƒ«è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ä½¿ç”¨æ–¹æ³•: ./download_models.sh

set -e  # ã‚¨ãƒ©ãƒ¼æ™‚ã«åœæ­¢

echo "ğŸ¤ GPT-SoVITS v4 ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹..."
echo ""

# å¿…è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
echo "ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆä¸­..."
mkdir -p pretrained_models/{chinese-hubert-base,chinese-roberta-wwm-ext-large,gsv-v4-pretrained}
mkdir -p {GPT_weights,SoVITS_weights,input,output,logs,reference,test_samples}

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢æ•°
download_file() {
    local url=$1
    local output=$2
    local description=$3
    local size=$4
    
    echo "ğŸ“¥ $description ($size) ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­..."
    echo "   URL: $url"
    echo "   å‡ºåŠ›: $output"
    
    if [ -f "$output" ]; then
        echo "   âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™: $output"
        return 0
    fi
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    mkdir -p "$(dirname "$output")"
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
    if wget -O "$output" "$url"; then
        echo "   âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: $output"
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        actual_size=$(du -h "$output" | cut -f1)
        echo "   ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: $actual_size"
    else
        echo "   âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: $output"
        rm -f "$output"  # å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        return 1
    fi
}

echo "ğŸ” å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™..."
echo "ğŸ“Š ç·ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚º: ç´„2.6GB"
echo ""

# 1. chinese-hubert-base (361MB)
echo "1ï¸âƒ£ Chinese HuBERT Base ãƒ¢ãƒ‡ãƒ«"
download_file \
    "https://huggingface.co/TencentGameMate/chinese-hubert-base/resolve/main/pytorch_model.bin" \
    "pretrained_models/chinese-hubert-base/pytorch_model.bin" \
    "Chinese HuBERT Base - PyTorch Model" \
    "361MB"

download_file \
    "https://huggingface.co/TencentGameMate/chinese-hubert-base/resolve/main/config.json" \
    "pretrained_models/chinese-hubert-base/config.json" \
    "Chinese HuBERT Base - Config" \
    "1KB"

download_file \
    "https://huggingface.co/TencentGameMate/chinese-hubert-base/resolve/main/preprocessor_config.json" \
    "pretrained_models/chinese-hubert-base/preprocessor_config.json" \
    "Chinese HuBERT Base - Preprocessor Config" \
    "1KB"

echo ""

# 2. chinese-roberta-wwm-ext-large (1.3GB)
echo "2ï¸âƒ£ Chinese RoBERTa WWM Ext Large ãƒ¢ãƒ‡ãƒ«"
download_file \
    "https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/pytorch_model.bin" \
    "pretrained_models/chinese-roberta-wwm-ext-large/pytorch_model.bin" \
    "Chinese RoBERTa WWM Ext Large - PyTorch Model" \
    "1.3GB"

download_file \
    "https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/config.json" \
    "pretrained_models/chinese-roberta-wwm-ext-large/config.json" \
    "Chinese RoBERTa WWM Ext Large - Config" \
    "1KB"

download_file \
    "https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/tokenizer.json" \
    "pretrained_models/chinese-roberta-wwm-ext-large/tokenizer.json" \
    "Chinese RoBERTa WWM Ext Large - Tokenizer" \
    "2MB"

download_file \
    "https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/tokenizer_config.json" \
    "pretrained_models/chinese-roberta-wwm-ext-large/tokenizer_config.json" \
    "Chinese RoBERTa WWM Ext Large - Tokenizer Config" \
    "1KB"

download_file \
    "https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/resolve/main/vocab.txt" \
    "pretrained_models/chinese-roberta-wwm-ext-large/vocab.txt" \
    "Chinese RoBERTa WWM Ext Large - Vocabulary" \
    "110KB"

echo ""

# 3. GPT-SoVITS v4 ãƒ—ãƒªãƒˆãƒ¬ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ« (790MB)
echo "3ï¸âƒ£ GPT-SoVITS v4 ãƒ—ãƒªãƒˆãƒ¬ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«"
download_file \
    "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/gsv-v4-pretrained/s2G.pth" \
    "pretrained_models/gsv-v4-pretrained/s2G.pth" \
    "GPT-SoVITS v4 - Generator Model" \
    "395MB"

download_file \
    "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/gsv-v4-pretrained/s2D.pth" \
    "pretrained_models/gsv-v4-pretrained/s2D.pth" \
    "GPT-SoVITS v4 - Discriminator Model" \
    "246MB"

download_file \
    "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/gsv-v4-pretrained/vocoder.pth" \
    "pretrained_models/gsv-v4-pretrained/vocoder.pth" \
    "GPT-SoVITS v4 - Vocoder Model" \
    "149MB"

echo ""

# 4. s1v3.ckpt (149MB)
echo "4ï¸âƒ£ GPT-SoVITS v4 S1 ãƒ¢ãƒ‡ãƒ«"
download_file \
    "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/s1v3.ckpt" \
    "pretrained_models/s1v3.ckpt" \
    "GPT-SoVITS v4 - S1 Checkpoint" \
    "149MB"

echo ""

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœç¢ºèª
echo "ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœç¢ºèª:"
echo ""

models=(
    "pretrained_models/chinese-hubert-base/pytorch_model.bin"
    "pretrained_models/chinese-roberta-wwm-ext-large/pytorch_model.bin"
    "pretrained_models/gsv-v4-pretrained/s2G.pth"
    "pretrained_models/gsv-v4-pretrained/s2D.pth"
    "pretrained_models/gsv-v4-pretrained/vocoder.pth"
    "pretrained_models/s1v3.ckpt"
)

success_count=0
total_count=${#models[@]}

for model in "${models[@]}"; do
    if [ -f "$model" ]; then
        size=$(du -h "$model" | cut -f1)
        echo "âœ… $model ($size)"
        ((success_count++))
    else
        echo "âŒ $model (è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)"
    fi
done

echo ""
echo "ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: $success_count/$total_count"

if [ $success_count -eq $total_count ]; then
    echo "ğŸ‰ å…¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼"
    
    # ç·ã‚µã‚¤ã‚ºè¨ˆç®—
    total_size=$(du -sh pretrained_models/ | cut -f1)
    echo "ğŸ“Š ç·ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚º: $total_size"
    
    echo ""
    echo "ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:"
    echo "  1. å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ reference/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®"
    echo "  2. Dockeræ§‹ç¯‰: docker compose -f docker-compose.fastapi.yml build"
    echo "  3. FastAPIèµ·å‹•: ./start_all_apis.sh"
    echo ""
    echo "âœ… GPT-SoVITS v4 ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼"
    
    exit 0
else
    echo "âš ï¸  ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ"
    echo "ğŸ”„ å†å®Ÿè¡Œã—ã¦ãã ã•ã„: ./download_models.sh"
    
    exit 1
fi 